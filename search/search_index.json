{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AAW Developer Docs The main purpose of this documentation site is to provide developer-facing documentation for large-scale platform features that are implemented across many repositories. Additionally, this site also contains information about useful developer tooling and other onboarding information that developers may wish to refer back to. Information about individual repoositories can be found in the README.md and associated documentation for those repositories. Repository Index TODO Repository Purpose Getting Started Instructions to connect to the AAW Kubernetes Clusters Attribution Several icons used in the diagrams throughout this documentation site were borrowed from third party resources. Attribution for each icon used is given below. Browser icons created by Pixel perfect - Flaticon Cog icons created by Freepik - Flaticon Gitlab icons provided by gitlab Github icons provided by Github Argo icon provided by Docker Moby (Docker) icon provided by Docker Terraform icon provided by Hashicorp Kubeflow icon provided by Kubeflow Istio icons provided by Istio Media Resources","title":"AAW Developer Docs"},{"location":"#aaw-developer-docs","text":"The main purpose of this documentation site is to provide developer-facing documentation for large-scale platform features that are implemented across many repositories. Additionally, this site also contains information about useful developer tooling and other onboarding information that developers may wish to refer back to. Information about individual repoositories can be found in the README.md and associated documentation for those repositories.","title":"AAW Developer Docs"},{"location":"#repository-index","text":"TODO Repository Purpose","title":"Repository Index"},{"location":"#getting-started","text":"Instructions to connect to the AAW Kubernetes Clusters","title":"Getting Started"},{"location":"#attribution","text":"Several icons used in the diagrams throughout this documentation site were borrowed from third party resources. Attribution for each icon used is given below. Browser icons created by Pixel perfect - Flaticon Cog icons created by Freepik - Flaticon Gitlab icons provided by gitlab Github icons provided by Github Argo icon provided by Docker Moby (Docker) icon provided by Docker Terraform icon provided by Hashicorp Kubeflow icon provided by Kubeflow Istio icons provided by Istio Media Resources","title":"Attribution"},{"location":"features/cloud-main-connectivity/cloud-main-connectivity/","text":"Overview Certain users of the AAW platform (e.g. StatCan employees) require access to certain services in our internal cloud environment (e.g. our internal gitlab instance). This documentation page describes the various mechanisms used to enable certain AAW namespaces connectivity to our cloud main environment. Related Repositories, Issues, and Pull Requests Repository: Profile State Controller Repository: Gatekeeper Constraints : exceptions of each category are specified in the parameters of the DenyExternalUsers constraint. Repository: Gatekeeper Policies : rego policies and tests for Issue: Non-Employee RBAC Model PR: UDR/Firewall PR to networking module Feature Deployment Note about Istio Logging : See Configuring Istio Logging for more information on how Istio logging is configured. Istio's default log level ( warning ) doesn't show the access logs for this feature, but if you set the log level to debug , you can confirm that the correct requests are, in fact, routed through the Istio Egress Gateway. Profile State Controller The profile-state-controller watches rolebindings in each kubeflow profile. If a profile only contains role bindings whose subjects' email domains are either statcan.gc.ca or cloud.statcan.ca , then the profile and corresponding namespace are given the label state.aaw.statcan.gc.ca/exists-non-cloud-main-user=false , indicating that there are no non-employee users present in the namespace. If one or more role bindings contain a subject with an email domain other than statcan.gc.ca or cloud.statcan.ca , the profile and namespace are given the label state.aaw.statcan.gc.ca/exists-non-cloud-main-user=true , indicating that there is at least one non-employee user with access to the namespace. Istio Egress Gateway The following diagrams illustrate the behaviour of the Istio Egress Gateway as well as the deployed components required for this example. The profiles-controller-cloud-main controller (deployed in the daaas-system namespace) watches namespaces and creates an Istio Virtual Service called cloud-main-virtualservice in all namespaces with the label state.aaw.statcan.gc.ca/exists-non-cloud-main-user=false , but not in namespaces with the label state.aaw.statcan.gc.ca/exists-non-cloud-main-user=true . The virtual service configures the envoy proxies of pods in the employee namespace to route outbound traffic with host matching a list of allowed hosts (e.g. gitlab.k8s.cloud.statcan.ca ) to the cloud-main-egress-gateway . A special cloud main node pool is created on the Kubernetes cluster and assigned a subnet with a distinct IP range . A dedicated Istio egress gateway pod is scheduled to a node from the cloud main node pool, which receives a pod IP from the subnet assigned to the cloud main node pool. This egress gateway pod IP comes from an IP range that is distinct from the subnet of user node pools. The virtual service deployed into employee-only namespaces configures the envoy proxies on each pod in the namespace to route traffic to gitlab.k8s.cloud.statcan.ca to the cloud-main-egress-gateway . Therefore, when pods running in employee-only namespaces (containing gitlab-service-entry ) initiate a request to gitlab.k8s.cloud.statcan.ca , the request is routed through the cloud-main-egress-gateway , which will have an outgoing IP associated with one of the cloud-main-system nodes. When pods running in namespaces that contain an external employee (not containing gitlab-service-entry ), requests to gitlab.k8s.cloud.statcan.ca will not be routed through the cloud-main-egress-gateway , and therefore will have an outgoing IP address associated with a user node. In the cloud main boundary firewall, a rule can be put in place to only allow incoming TCP traffic on ports 443 or 22 originating from the cloud-main-egress-gateway IP address; requests originating from user pod IPs will be blocked at the firewall level. In each employee-only namespace, network policies must be added that allow egress to the cloud-main-system namespace. A corresponding network policy based on a namespace label selector (see e.g. description posted in https://github.com/StatCan/daaas/issues/1097#issue-1234409276) must be added to the cloud-main-system namespace to allow ingress from employee-only namespaces. The green arrow in the diagram below shows the route taken by requests to gitlab.k8s.cloud.statcan.ca from employee-only namespaces, while the red arrow shows the route taken by requests to gitlab.k8s.cloud.statcan.ca from namespaces with at least one non-employee users. Azure Networking Several Azure components need to be configured through Terraform. TODO Attribution Jupyter Icon borrowed from wikimedia commons","title":"Cloud Main Connectivity"},{"location":"features/cloud-main-connectivity/cloud-main-connectivity/#overview","text":"Certain users of the AAW platform (e.g. StatCan employees) require access to certain services in our internal cloud environment (e.g. our internal gitlab instance). This documentation page describes the various mechanisms used to enable certain AAW namespaces connectivity to our cloud main environment.","title":"Overview"},{"location":"features/cloud-main-connectivity/cloud-main-connectivity/#related-repositories-issues-and-pull-requests","text":"Repository: Profile State Controller Repository: Gatekeeper Constraints : exceptions of each category are specified in the parameters of the DenyExternalUsers constraint. Repository: Gatekeeper Policies : rego policies and tests for Issue: Non-Employee RBAC Model PR: UDR/Firewall PR to networking module","title":"Related Repositories, Issues, and Pull Requests"},{"location":"features/cloud-main-connectivity/cloud-main-connectivity/#feature-deployment","text":"Note about Istio Logging : See Configuring Istio Logging for more information on how Istio logging is configured. Istio's default log level ( warning ) doesn't show the access logs for this feature, but if you set the log level to debug , you can confirm that the correct requests are, in fact, routed through the Istio Egress Gateway.","title":"Feature Deployment"},{"location":"features/cloud-main-connectivity/cloud-main-connectivity/#profile-state-controller","text":"The profile-state-controller watches rolebindings in each kubeflow profile. If a profile only contains role bindings whose subjects' email domains are either statcan.gc.ca or cloud.statcan.ca , then the profile and corresponding namespace are given the label state.aaw.statcan.gc.ca/exists-non-cloud-main-user=false , indicating that there are no non-employee users present in the namespace. If one or more role bindings contain a subject with an email domain other than statcan.gc.ca or cloud.statcan.ca , the profile and namespace are given the label state.aaw.statcan.gc.ca/exists-non-cloud-main-user=true , indicating that there is at least one non-employee user with access to the namespace.","title":"Profile State Controller"},{"location":"features/cloud-main-connectivity/cloud-main-connectivity/#istio-egress-gateway","text":"The following diagrams illustrate the behaviour of the Istio Egress Gateway as well as the deployed components required for this example. The profiles-controller-cloud-main controller (deployed in the daaas-system namespace) watches namespaces and creates an Istio Virtual Service called cloud-main-virtualservice in all namespaces with the label state.aaw.statcan.gc.ca/exists-non-cloud-main-user=false , but not in namespaces with the label state.aaw.statcan.gc.ca/exists-non-cloud-main-user=true . The virtual service configures the envoy proxies of pods in the employee namespace to route outbound traffic with host matching a list of allowed hosts (e.g. gitlab.k8s.cloud.statcan.ca ) to the cloud-main-egress-gateway . A special cloud main node pool is created on the Kubernetes cluster and assigned a subnet with a distinct IP range . A dedicated Istio egress gateway pod is scheduled to a node from the cloud main node pool, which receives a pod IP from the subnet assigned to the cloud main node pool. This egress gateway pod IP comes from an IP range that is distinct from the subnet of user node pools. The virtual service deployed into employee-only namespaces configures the envoy proxies on each pod in the namespace to route traffic to gitlab.k8s.cloud.statcan.ca to the cloud-main-egress-gateway . Therefore, when pods running in employee-only namespaces (containing gitlab-service-entry ) initiate a request to gitlab.k8s.cloud.statcan.ca , the request is routed through the cloud-main-egress-gateway , which will have an outgoing IP associated with one of the cloud-main-system nodes. When pods running in namespaces that contain an external employee (not containing gitlab-service-entry ), requests to gitlab.k8s.cloud.statcan.ca will not be routed through the cloud-main-egress-gateway , and therefore will have an outgoing IP address associated with a user node. In the cloud main boundary firewall, a rule can be put in place to only allow incoming TCP traffic on ports 443 or 22 originating from the cloud-main-egress-gateway IP address; requests originating from user pod IPs will be blocked at the firewall level. In each employee-only namespace, network policies must be added that allow egress to the cloud-main-system namespace. A corresponding network policy based on a namespace label selector (see e.g. description posted in https://github.com/StatCan/daaas/issues/1097#issue-1234409276) must be added to the cloud-main-system namespace to allow ingress from employee-only namespaces. The green arrow in the diagram below shows the route taken by requests to gitlab.k8s.cloud.statcan.ca from employee-only namespaces, while the red arrow shows the route taken by requests to gitlab.k8s.cloud.statcan.ca from namespaces with at least one non-employee users.","title":"Istio Egress Gateway"},{"location":"features/cloud-main-connectivity/cloud-main-connectivity/#azure-networking","text":"Several Azure components need to be configured through Terraform. TODO","title":"Azure Networking"},{"location":"features/cloud-main-connectivity/cloud-main-connectivity/#attribution","text":"Jupyter Icon borrowed from wikimedia commons","title":"Attribution"},{"location":"features/object-storage/blobcsi/","text":"Overview The blob-csi system is the replacement system for the minio storage solution in AAW . In order to provide AAW users with access to Azure storage containers, we deploy a storage solution consisting of the following components: azure blob-csi-driver : responsible for mounting PersistentVolume resources to azure storage containers via blob-fuse deployed by argocd into AAW here blob-csi-injector : MutatingWebhook which is responsible for adding Volumes and VolumeMounts to pods containing the data.statcan.gc.ca/inject-blob-volumes label. deployed by argocd into AAW here blob-csi.go kubernetes controller : Responsible for the provisioning of Azure storage containers, PersistentVolume s and PersistentVolumeClaim s per namespace for AAW users. deployed by argocd into AAW here Feature Implementation The blob-csi system allows users to gain access to Azure container storage at unclassified and protected-b classification levels. Currently, users can mount volumes manually as data-volumes in AAW's Kubeflow notebook creation workflow, but eventually auto-mounting of volumes will be supported. Note that the figures depicted below are representative of the AAW development Kubernetes cluster. However; the implementation in production is near identical, with the only difference being the use of the word dev being replaced with prod throughout the resources. The data flow for how a kubeflow notebook will connect to backing storage at a high level is provided below. In short, users request a data volume for use within their kubeflow notebook, the blob-csi-driver runs a csi-controller pod on each node in the cluster, and each csi-controller pod mounts any volumes requested. The blob-csi.go Kubernetes Controller The blob-csi.go kubernetes controller is responsible for the creation of PersistentVolume , PersistentVolumeClaim and azure storage containers per user namespace in AAW. In addition, the controller must monitor a permissions list obtained through OPA gateways which serve a list of azure containers and who can be a reader/writer for each container. The permissions lists are managed by Fair Data Infrastructure team (FDI) . AAW default volumes are configured here and any change to the name field will result in the deletion and re-creation of PersistentVolume and PersistentVolumeClaim resources to reconcile the change. There are two possible classifications for volumes: unclassified and protected-b . Notice that the unclassified-ro classification is protected-b . This is so that users can view unclassified data within a protected-b pod (volume classifications are enforced by Gatekeeper upon creation of a notebook). In order for the controller to determine which PersistentVolume s and PersistentVolumeClaim s to create for FDI containers, the controller queries unclassified and protected-b OPA gateways pod s via http requests, recieving a .json formatted response. An example of the expected .json formatted response from an OPA gateway is included below: { \"container1/SubfolderName\": { \"name\" : \"name-of-pvc\", \"readers\" : [\"alice\", \"bob\"], \"writers\" : [\"bob\"], \"spn\" : \"name-of-spn\" } } In the above example, the controller would provision a PersistentVolume and PersistentVolumeClaim for both alice and bob , however alice would have ReadOnlyMany permissions, and bob would have ReadWriteMany permissions. There is also an option to mount subfolders to a user's container. Architecture Design For more context on the blob-csi system as a whole (from deployment of infrastructure to azure containers), see the attached diagram below. In addition, refer to the below legend for line types and colours. Line types: Solid lines follow the paths of the deployment or provisioning of resources. Dashed lines outline the paths for connectivity. Line Colours: Navy blue lines are used for edges connecting nodes within the kubernetes cluster. Light green lines are assiciated with kubeflow from a users perspective. Yellow lines are associated with argocd. Purple lines are associated with terraform. Light blue lines are associated with edges that connect from nodes anywhere in the diagram to an azure resource.","title":"Blob CSI"},{"location":"features/object-storage/blobcsi/#overview","text":"The blob-csi system is the replacement system for the minio storage solution in AAW . In order to provide AAW users with access to Azure storage containers, we deploy a storage solution consisting of the following components: azure blob-csi-driver : responsible for mounting PersistentVolume resources to azure storage containers via blob-fuse deployed by argocd into AAW here blob-csi-injector : MutatingWebhook which is responsible for adding Volumes and VolumeMounts to pods containing the data.statcan.gc.ca/inject-blob-volumes label. deployed by argocd into AAW here blob-csi.go kubernetes controller : Responsible for the provisioning of Azure storage containers, PersistentVolume s and PersistentVolumeClaim s per namespace for AAW users. deployed by argocd into AAW here","title":"Overview"},{"location":"features/object-storage/blobcsi/#feature-implementation","text":"The blob-csi system allows users to gain access to Azure container storage at unclassified and protected-b classification levels. Currently, users can mount volumes manually as data-volumes in AAW's Kubeflow notebook creation workflow, but eventually auto-mounting of volumes will be supported. Note that the figures depicted below are representative of the AAW development Kubernetes cluster. However; the implementation in production is near identical, with the only difference being the use of the word dev being replaced with prod throughout the resources. The data flow for how a kubeflow notebook will connect to backing storage at a high level is provided below. In short, users request a data volume for use within their kubeflow notebook, the blob-csi-driver runs a csi-controller pod on each node in the cluster, and each csi-controller pod mounts any volumes requested.","title":"Feature Implementation"},{"location":"features/object-storage/blobcsi/#the-blob-csigo-kubernetes-controller","text":"The blob-csi.go kubernetes controller is responsible for the creation of PersistentVolume , PersistentVolumeClaim and azure storage containers per user namespace in AAW. In addition, the controller must monitor a permissions list obtained through OPA gateways which serve a list of azure containers and who can be a reader/writer for each container. The permissions lists are managed by Fair Data Infrastructure team (FDI) . AAW default volumes are configured here and any change to the name field will result in the deletion and re-creation of PersistentVolume and PersistentVolumeClaim resources to reconcile the change. There are two possible classifications for volumes: unclassified and protected-b . Notice that the unclassified-ro classification is protected-b . This is so that users can view unclassified data within a protected-b pod (volume classifications are enforced by Gatekeeper upon creation of a notebook). In order for the controller to determine which PersistentVolume s and PersistentVolumeClaim s to create for FDI containers, the controller queries unclassified and protected-b OPA gateways pod s via http requests, recieving a .json formatted response. An example of the expected .json formatted response from an OPA gateway is included below: { \"container1/SubfolderName\": { \"name\" : \"name-of-pvc\", \"readers\" : [\"alice\", \"bob\"], \"writers\" : [\"bob\"], \"spn\" : \"name-of-spn\" } } In the above example, the controller would provision a PersistentVolume and PersistentVolumeClaim for both alice and bob , however alice would have ReadOnlyMany permissions, and bob would have ReadWriteMany permissions. There is also an option to mount subfolders to a user's container.","title":"The blob-csi.go Kubernetes Controller"},{"location":"features/object-storage/blobcsi/#architecture-design","text":"For more context on the blob-csi system as a whole (from deployment of infrastructure to azure containers), see the attached diagram below. In addition, refer to the below legend for line types and colours. Line types: Solid lines follow the paths of the deployment or provisioning of resources. Dashed lines outline the paths for connectivity. Line Colours: Navy blue lines are used for edges connecting nodes within the kubernetes cluster. Light green lines are assiciated with kubeflow from a users perspective. Yellow lines are associated with argocd. Purple lines are associated with terraform. Light blue lines are associated with edges that connect from nodes anywhere in the diagram to an azure resource.","title":"Architecture Design"},{"location":"features/object-storage/s3proxy/","text":"Overview In order to provide users of the platform with an S3 interface, we use a fork of aws-js-s3-explorer in combination with s3proxy . s3Proxy runs in \"filesystem\" mode, where the filesystem volumes provided by the blob fuse CSI driver serve as a data back-end, while the aws-js-s3-explorer provides a client-side web application for users to explore the contents of their S3 buckets. Relevant Repositories aws-js-s3-explorer aaw-kubeflow-profiles-controller aaw-argocd-manifests charts Feature Deployment The S3Proxy application is deployed on an opt-in basis to reduce resource consumption in the cluster. The majority of users only require filesystem access to Azure blob storage, which is already provided by the blob fuse CSI driver ; it is only necessary to deploy this s3proxy application if users explicitly require an s3 interface to Azure blob storage. To enable s3proxy in a namespace, go to the aaw-kubeflow-profiles repository, go to the specific *.jsonnet file that contains the profile definition, then wrap the profile definition in the addS3 function. See this example for a profile where both s3proxy and Gitea were added to the namespace. If the user profile has the label s3.statcan.gc.ca/enabled: true , then the s3proxy profiles controller will install an ArgoCD application, an nginx configmap, and an Istio virtual service into the user's namespace. The ArgoCD application deploys a number of resources to the user's namespace via jsonnet. The ArgoCD location watches the /profiles-argocd-system/template/s3proxy location of the aaw-argocd-manifests repo for changes to the aaw-dev-cc-00 or aaw-prod-cc-00 branches for the dev and prod deployments, respectively. Note : there are two network policies deployed by the per-namespace ArgoCD application (manifests can be found in the application.jsonnet file at the repo location linked above). These network policies are required in order to allow users to connect to the protected-b s3proxy instance from a noVNC protected-b notebook. Feature Implementation The way users connect to the s3 explorer depends on whether they are working in an unclassified environment or a protected-b environment. In an unclassified environment, the architecture is as shown in the figure below. Users connect to s3proxy through the Kubeflow user interface, and the s3proxy web application renders inside of an iframe in the Kubeflow User interface. All outgoing requests from the page are intercepted by a service worker - a client-side middleware that can intercept and modify outgoing requests before they are sent to the server. An Istio virtual service applies routing logic to incoming requests based on uri prefix patterns and request headers. Requests are directed towards the s3proxy-web service, which routes requests to s3proxy pods. Inside of s3proxy pods, there is an nginx container that hosts the static files associated with the aws-js-s3-explorer, and also proxies requests for s3proxy . The s3proxy application runs in \"filesystem\" mode, so all s3 API calls made against s3proxy are translated into the appropriate system call to perform operations against the filesystem. In particular, each bucket provided by s3proxy is a volume that is mounted to the s3proxy container filesystem by the blob fuse CSI driver. In a protected B environment, user access is slightly different as shown in the diagram below. Unlike the unclassified setup, users cannot access the aws-js-s3-explorer user interface through the Kubeflow interface. Rather, users can only access the aws-js-s3-explorer application through a browser in a noVNC notebook. Under this setup, users have a VDI connection to their noVNC notebook, so neither the service worker nor the virtual service configuration are applicable; rather, the noVNC pod makes requests directly against the s3proxy-web service in the same namespace. Service Worker Explanation As far as we can tell, the AWS client used by aws-js-s3-explorer does not allow for unauthenticated requests. In other words, the client requires an access key/secret key pair. The AWS javascript client automatically generates a corresponding authorization request header for all AJAX calls made by the application. Since each namespace has its own private s3proxy instance, and users accessing Kubeflow have already authenticated, it would be redundant to have an additional authentication layer in front of s3proxy. To get around this upstream requirement, a fake access key/secret key are created on the client side so that the aws javascript client can successfully make requests, and each user's private s3proxy instance runs with no authentication required. The workaround described above is sufficient for users accessing s3-proxy through the noVNC pod since the request is made in the cluster. However, users accessing the aws-s3-js-explorer through the Kubeflow interface pass through an authenticated ingress gateway. When requests enter through this gateway, they are rejected due to the erroneous authorization header. To avoid significantly refactoring the aws-js-s3-explorer code, a service worker intercepts requests and removes the erroneous authorization header. There are 4 cases that occur in the fetch listener of the sw_modify_header.js service worker. Case 1: do not modify requests that are not directed to s3proxy Endpoints with prefixes other than /unclassified are not AJAX calls made by the AWS s3 client, and do not have the erroneous authorization header. All such requests are simply forwarded by the service worker without any modification. Case 2: remove auth header in requests with methods other than PUT and POST AJAX calls that are not PUT or POST only need the erroneous authorization header removed (e.g. GET requests to list objects in a bucket, DELETE request to remove an object, etc.). No further action is required, so the modified request is forwarded to s3proxy . Case 3: remove auth header and create .empty file if a new folder is being created Any PUT requests with a URI that ends with / are requests to create a new folder. As the concept of folder doesn't exist in s3, requests of this type are intercepted and an empty file called .empty is attached to the request so that the prefix pattern is preserved. For example, a request to create a folder called new-folder in the unclassified bucket would have a request with the prefix /unclassified/new-folder/ , so the service worker would intercept this request and send the request /unclassified/new-folder/.empty so that the user can add files to the new location. Case 4: remove auth header and forward request body to s3proxy In the final case, there is a PUT or POST request to upload a file to s3proxy . In this case, the service worker creates an ArrayBuffer object for the request body data and passes the request onto s3proxy . This last case is required because the body of PUT and POST requests is not included in the event.request object that the service worker receives, so the request body must be loaded into a transferrable object before being sent to s3proxy . Request Routing An istio virtual service and nginx configuration are used to ensure that requests from the Kubeflow user interface arrive correctly at the Nginx/s3proxy servers. This section details how the virtual service / nginx configuration modify requests. (1) Getting the index.html page A request is sent from the page https://kubeflow.aaw.cloud.statcan.ca/s3/?ns=<user namespace> (i.e. the referer header is https://kubeflow.aaw.cloud.statcan.ca/s3/?ns=<user namespace> ). The istio virtual service in the user's namespace has a routing rule for requests with this referer header, and redirects the request to the uri /s3/aaw-fc/index.html . Another route rule catches requests with the URI prefix /s3/aaw-fc/ and routes them to s3proxy-web.<user namespace>.svc.cluster.local . The index.html file is returned and the base URL of the page is https://kubeflow.aaw.cloud.statcan.ca/s3/<user namespace>/ (i.e. all requests initiated from the index.html page will be addressed to https://kubeflow.aaw.cloud.statcan.ca/s3/<user namespace>/ followed by the path to the static file being requested). (2) Getting the static files (application and vendor) Since the static files need to be accessible from a protected-b noVNC notebook without internet access, all static files are served from the Nginx static file server. The Dockerfile in the aws-js-s3-explorer repository copies all of the static files (application and vendor) to a location on the container filesystem. The s3proxy pod contains an init job that copies the static files from the s3proxy image to a known location on the Nginx container file system so they can be served from the Nginx container. All static files contain the prefix /s3/aaw-fc/ (see explanation in (1)), so the same virtual service route that routes the request for index.html to s3proxy-web.<user namespace>.svc.cluster.local routes the requests for all static files to this service. (3) AJAX requests initiated from AWS client The AWS javascript client is unaware of the /s3/aaw-fc/ prefix that was added to the requests for static files. As such, it initiates requests of the form https://kubeflow.aaw.cloud.statcan.ca/<bucket name>/<object prefix> , which does not have enough information on its own to route the request to a specific user's namespace. However, since all outgoing requests are caught by the service worker, the referer for outgoing AJAX calls is https://kubeflow.aaw.cloud.statcan.ca/s3/<user namespace>/sw_modify_header.js , which can be used to route the AJAX requests to the correct user's s3proxy.<user namespace>.svc.cluster.local service. (4) Downloading files from aws-js-s3-explorer If a user tries to download a file by clicking one of the links in the aws-js-s3-explorer, the referer url is https://kubeflow.aaw-dev.cloud.statcan.ca/s3/aaw-fc/index.html instead of https://kubeflow.aaw.cloud.statcan.ca/s3/<user namespace>/sw_modify_header.js since we forward the existing request instead of initiating a new request from the service worker . In this case, the request is also directed to the user's s3proxy.<user namespace>.svc.cluster.local service, and the file will be downloaded to the user's browser.","title":"S3Proxy and S3 Explorer"},{"location":"features/object-storage/s3proxy/#overview","text":"In order to provide users of the platform with an S3 interface, we use a fork of aws-js-s3-explorer in combination with s3proxy . s3Proxy runs in \"filesystem\" mode, where the filesystem volumes provided by the blob fuse CSI driver serve as a data back-end, while the aws-js-s3-explorer provides a client-side web application for users to explore the contents of their S3 buckets.","title":"Overview"},{"location":"features/object-storage/s3proxy/#relevant-repositories","text":"aws-js-s3-explorer aaw-kubeflow-profiles-controller aaw-argocd-manifests charts","title":"Relevant Repositories"},{"location":"features/object-storage/s3proxy/#feature-deployment","text":"The S3Proxy application is deployed on an opt-in basis to reduce resource consumption in the cluster. The majority of users only require filesystem access to Azure blob storage, which is already provided by the blob fuse CSI driver ; it is only necessary to deploy this s3proxy application if users explicitly require an s3 interface to Azure blob storage. To enable s3proxy in a namespace, go to the aaw-kubeflow-profiles repository, go to the specific *.jsonnet file that contains the profile definition, then wrap the profile definition in the addS3 function. See this example for a profile where both s3proxy and Gitea were added to the namespace. If the user profile has the label s3.statcan.gc.ca/enabled: true , then the s3proxy profiles controller will install an ArgoCD application, an nginx configmap, and an Istio virtual service into the user's namespace. The ArgoCD application deploys a number of resources to the user's namespace via jsonnet. The ArgoCD location watches the /profiles-argocd-system/template/s3proxy location of the aaw-argocd-manifests repo for changes to the aaw-dev-cc-00 or aaw-prod-cc-00 branches for the dev and prod deployments, respectively. Note : there are two network policies deployed by the per-namespace ArgoCD application (manifests can be found in the application.jsonnet file at the repo location linked above). These network policies are required in order to allow users to connect to the protected-b s3proxy instance from a noVNC protected-b notebook.","title":"Feature Deployment"},{"location":"features/object-storage/s3proxy/#feature-implementation","text":"The way users connect to the s3 explorer depends on whether they are working in an unclassified environment or a protected-b environment. In an unclassified environment, the architecture is as shown in the figure below. Users connect to s3proxy through the Kubeflow user interface, and the s3proxy web application renders inside of an iframe in the Kubeflow User interface. All outgoing requests from the page are intercepted by a service worker - a client-side middleware that can intercept and modify outgoing requests before they are sent to the server. An Istio virtual service applies routing logic to incoming requests based on uri prefix patterns and request headers. Requests are directed towards the s3proxy-web service, which routes requests to s3proxy pods. Inside of s3proxy pods, there is an nginx container that hosts the static files associated with the aws-js-s3-explorer, and also proxies requests for s3proxy . The s3proxy application runs in \"filesystem\" mode, so all s3 API calls made against s3proxy are translated into the appropriate system call to perform operations against the filesystem. In particular, each bucket provided by s3proxy is a volume that is mounted to the s3proxy container filesystem by the blob fuse CSI driver. In a protected B environment, user access is slightly different as shown in the diagram below. Unlike the unclassified setup, users cannot access the aws-js-s3-explorer user interface through the Kubeflow interface. Rather, users can only access the aws-js-s3-explorer application through a browser in a noVNC notebook. Under this setup, users have a VDI connection to their noVNC notebook, so neither the service worker nor the virtual service configuration are applicable; rather, the noVNC pod makes requests directly against the s3proxy-web service in the same namespace.","title":"Feature Implementation"},{"location":"features/object-storage/s3proxy/#service-worker-explanation","text":"As far as we can tell, the AWS client used by aws-js-s3-explorer does not allow for unauthenticated requests. In other words, the client requires an access key/secret key pair. The AWS javascript client automatically generates a corresponding authorization request header for all AJAX calls made by the application. Since each namespace has its own private s3proxy instance, and users accessing Kubeflow have already authenticated, it would be redundant to have an additional authentication layer in front of s3proxy. To get around this upstream requirement, a fake access key/secret key are created on the client side so that the aws javascript client can successfully make requests, and each user's private s3proxy instance runs with no authentication required. The workaround described above is sufficient for users accessing s3-proxy through the noVNC pod since the request is made in the cluster. However, users accessing the aws-s3-js-explorer through the Kubeflow interface pass through an authenticated ingress gateway. When requests enter through this gateway, they are rejected due to the erroneous authorization header. To avoid significantly refactoring the aws-js-s3-explorer code, a service worker intercepts requests and removes the erroneous authorization header. There are 4 cases that occur in the fetch listener of the sw_modify_header.js service worker. Case 1: do not modify requests that are not directed to s3proxy Endpoints with prefixes other than /unclassified are not AJAX calls made by the AWS s3 client, and do not have the erroneous authorization header. All such requests are simply forwarded by the service worker without any modification. Case 2: remove auth header in requests with methods other than PUT and POST AJAX calls that are not PUT or POST only need the erroneous authorization header removed (e.g. GET requests to list objects in a bucket, DELETE request to remove an object, etc.). No further action is required, so the modified request is forwarded to s3proxy . Case 3: remove auth header and create .empty file if a new folder is being created Any PUT requests with a URI that ends with / are requests to create a new folder. As the concept of folder doesn't exist in s3, requests of this type are intercepted and an empty file called .empty is attached to the request so that the prefix pattern is preserved. For example, a request to create a folder called new-folder in the unclassified bucket would have a request with the prefix /unclassified/new-folder/ , so the service worker would intercept this request and send the request /unclassified/new-folder/.empty so that the user can add files to the new location. Case 4: remove auth header and forward request body to s3proxy In the final case, there is a PUT or POST request to upload a file to s3proxy . In this case, the service worker creates an ArrayBuffer object for the request body data and passes the request onto s3proxy . This last case is required because the body of PUT and POST requests is not included in the event.request object that the service worker receives, so the request body must be loaded into a transferrable object before being sent to s3proxy .","title":"Service Worker Explanation"},{"location":"features/object-storage/s3proxy/#request-routing","text":"An istio virtual service and nginx configuration are used to ensure that requests from the Kubeflow user interface arrive correctly at the Nginx/s3proxy servers. This section details how the virtual service / nginx configuration modify requests. (1) Getting the index.html page A request is sent from the page https://kubeflow.aaw.cloud.statcan.ca/s3/?ns=<user namespace> (i.e. the referer header is https://kubeflow.aaw.cloud.statcan.ca/s3/?ns=<user namespace> ). The istio virtual service in the user's namespace has a routing rule for requests with this referer header, and redirects the request to the uri /s3/aaw-fc/index.html . Another route rule catches requests with the URI prefix /s3/aaw-fc/ and routes them to s3proxy-web.<user namespace>.svc.cluster.local . The index.html file is returned and the base URL of the page is https://kubeflow.aaw.cloud.statcan.ca/s3/<user namespace>/ (i.e. all requests initiated from the index.html page will be addressed to https://kubeflow.aaw.cloud.statcan.ca/s3/<user namespace>/ followed by the path to the static file being requested). (2) Getting the static files (application and vendor) Since the static files need to be accessible from a protected-b noVNC notebook without internet access, all static files are served from the Nginx static file server. The Dockerfile in the aws-js-s3-explorer repository copies all of the static files (application and vendor) to a location on the container filesystem. The s3proxy pod contains an init job that copies the static files from the s3proxy image to a known location on the Nginx container file system so they can be served from the Nginx container. All static files contain the prefix /s3/aaw-fc/ (see explanation in (1)), so the same virtual service route that routes the request for index.html to s3proxy-web.<user namespace>.svc.cluster.local routes the requests for all static files to this service. (3) AJAX requests initiated from AWS client The AWS javascript client is unaware of the /s3/aaw-fc/ prefix that was added to the requests for static files. As such, it initiates requests of the form https://kubeflow.aaw.cloud.statcan.ca/<bucket name>/<object prefix> , which does not have enough information on its own to route the request to a specific user's namespace. However, since all outgoing requests are caught by the service worker, the referer for outgoing AJAX calls is https://kubeflow.aaw.cloud.statcan.ca/s3/<user namespace>/sw_modify_header.js , which can be used to route the AJAX requests to the correct user's s3proxy.<user namespace>.svc.cluster.local service. (4) Downloading files from aws-js-s3-explorer If a user tries to download a file by clicking one of the links in the aws-js-s3-explorer, the referer url is https://kubeflow.aaw-dev.cloud.statcan.ca/s3/aaw-fc/index.html instead of https://kubeflow.aaw.cloud.statcan.ca/s3/<user namespace>/sw_modify_header.js since we forward the existing request instead of initiating a new request from the service worker . In this case, the request is also directed to the user's s3proxy.<user namespace>.svc.cluster.local service, and the file will be downloaded to the user's browser.","title":"Request Routing"},{"location":"features/rbac/non-employee-rbac/","text":"Overview Some non-employee users of the AAW platform are authorized to perform some (but not all) of the capabilities that Statcan employees can perform. The platform components detailed on this page allow administrators to extend certain capabilities to certain non-employee users who are authorized to perform those capabilities. The two main capabilities that are relevant are (1) the ability to create pods with certain base images (e.g. SAS), or (2) the ability to access certain cloud main resources (e.g. gitlab.k8s). The documentation below focuses on these two types of capabilities; additional controls may be required for capabilities that don't fall under the abovementioned types. How to Add/Remove External Users Update the exception-list configmap Update the external users gatekeeper constraint Sync exception-list configmap on ArgoCD Restart profiles state controller deployment on ArgoCD (this is needed because we need a pod restart on the profile state controller to re-mount the updated configmap) Relevant Issues Refactor Non-Employee RBAC Model Relevant Repositories aaw-profile-state-controller aaw-kubeflow-profiles-controller gatekeeper-policies aaw-gatekeeper-constraints jupyter-apis aaw-kubeflow-profiles aaw-network-policies How to Add/Remove Users from Capabilities Go to the exception-list configmap and add/remove users from the desired exception list category. In general, the format of the exception list is as follows: # ... exceptionKind: - exceptionUser1@email.com - exceptionUser2@email.com # ... How to Add a New Capability Create a new exception kind in the exception-list configmap . Make the appropriate changes to the aaw-profile-state-controller repository. Add any unit tests to the changes in aaw-profile-state-controller to ensure correct behaviour. Depending on whether the exception kind is a pod/notebook capability or a namespace capability (described later on this page), the next steps differ. If the feature is a namespace capability, whatever controllers are involved with rolling out components for the capability should respond accordingly to changes in labels applied by aaw-profile-state-controller . For example, the cloud-main controller that permits authorized namespaces to connect to certain cloud-main services operates by creating certain Istio and Kubernetes resources in those namespaces to ensure that traffic from those namespaces is routed through the cloud main egress gateway. If a namespace adds an unauthorized user, the aaw-profile-state-controller applies the label state.aaw.statcan.gc.ca/exists-non-cloud-main-user: true , and the cloud-main controller and per-namespace network policy controller remove the necessary Istio and Kubernetes components from that namespace. As long as the unauthorized user is in the namespace, traffic from that namespace does not get routed through the cloud-main egress gateway, and is subsequently blocked at the AAW Hub firewall level if an attempt is made to communicate with cloud main services. If the feature is a pod/notebook capability, there are often two cases that need to be handled explicitly by a Gatekeeper policy: A namespace already contains a pod with feature X and an unauthorized user is added to the namespace. A namespace already contains an unauthorized user and a pod/notebook with feature X is created in the namespace. A Gatekeeper policy/constraint must be created for both cases so that the above scenarios are blocked at validating admission control (i.e. before the pod/notebook/rolebinding is posed to etcd). In addition, it may also be necessary to make changes to the jupyter-apis front-end to indicate on the UI that users are not allowed to perform certain actions. For example, for the SAS Notebook feature of the platform, the option to select a SAS notebook through the jupyter-apis UI is disabled if there exists a user without the SAS Notebook capability in the namespace. Feature Deployment The aaw-profile-state-controller is deployed in the daaas-system ArgoCD application. The manifests for the aaw-profile-state-controller deployment can be found in the daaas-system folder in the aaw-argocd-manifests repository. The [aaw-profile-state-controller] repository has a github action that builds and pushes the image for the profile-state-controller application to our AAW artifactory instance. The container image is tagged with the SHA of the commit on the master branch that triggered the github action. To update the image, go to the aaw-argocd-manifests/daaas-system/profile-state-controller/deployment.yaml and update the image tag to contain the SHA of the most recent commit in the aaw-profile-state-controller repo. Relevant Pull Requests This section shows the relevant pull requests that were involved with rolling out this feature into the AAW dev environment. Future updates to this feature will likely require similar pull requests to those shown below. Feature Implementation This implementation proposes two kinds of feature capabilities: (1) Pod/Notebook features and (2) Namespace features. For example, the SAS notebook feature is a Pod/notebook feature because it requires deploying a specific kind of notebook into a namespace, whereas cloud-main-connectivity is a namespace feature because routing pod traffic through the egress gateway is determined by rules that are applied at the level of the namespace. The semantics for labels behind each feature work as follows. Pod/Notebook Feature The profile state controller applies a label of the form has-X-feature if any pod/notebook in the namespace has that feature (e.g. a pod with the sas image). Additionally, the profile state controller applies a label of the form exists-non-X-user if any subject in any rolebinding in the namespace is not an employee or not in the list of exceptions for that capability. The profiles state controller will apply the following labels to the profile and namespace: has-X-feature: true if any pod in the namespace has that feature exists-non-X-user: true if any subject in any role binding is not an employee or is not in the exception list for that capability. If a non-employee without an exception is added to a rolebinding in a namespace where the label has-X-feature: true is present, a gatekeeper policy blocks this request. If a pod with X-feature is added to a namespace where the label exists-non-X-user is present, a gatekeeper policy blocks this request. Namespace Feature A namespace feature only requires the logic surrounding the exists-non-X-user label described above. In the case of cloud-main connectivity, the cloud-main and network controllers in aaw-kubeflow-profiles-controller should automatically reconcile the network policies / virtual services involved if a namespace does not have the cloud main connectivity capability. No gatekeeper policy should be required.","title":"Non-Employee RBAC"},{"location":"features/rbac/non-employee-rbac/#overview","text":"Some non-employee users of the AAW platform are authorized to perform some (but not all) of the capabilities that Statcan employees can perform. The platform components detailed on this page allow administrators to extend certain capabilities to certain non-employee users who are authorized to perform those capabilities. The two main capabilities that are relevant are (1) the ability to create pods with certain base images (e.g. SAS), or (2) the ability to access certain cloud main resources (e.g. gitlab.k8s). The documentation below focuses on these two types of capabilities; additional controls may be required for capabilities that don't fall under the abovementioned types.","title":"Overview"},{"location":"features/rbac/non-employee-rbac/#how-to-addremove-external-users","text":"Update the exception-list configmap Update the external users gatekeeper constraint Sync exception-list configmap on ArgoCD Restart profiles state controller deployment on ArgoCD (this is needed because we need a pod restart on the profile state controller to re-mount the updated configmap)","title":"How to Add/Remove External Users"},{"location":"features/rbac/non-employee-rbac/#relevant-issues","text":"Refactor Non-Employee RBAC Model","title":"Relevant Issues"},{"location":"features/rbac/non-employee-rbac/#relevant-repositories","text":"aaw-profile-state-controller aaw-kubeflow-profiles-controller gatekeeper-policies aaw-gatekeeper-constraints jupyter-apis aaw-kubeflow-profiles aaw-network-policies","title":"Relevant Repositories"},{"location":"features/rbac/non-employee-rbac/#how-to-addremove-users-from-capabilities","text":"Go to the exception-list configmap and add/remove users from the desired exception list category. In general, the format of the exception list is as follows: # ... exceptionKind: - exceptionUser1@email.com - exceptionUser2@email.com # ...","title":"How to Add/Remove Users from Capabilities"},{"location":"features/rbac/non-employee-rbac/#how-to-add-a-new-capability","text":"Create a new exception kind in the exception-list configmap . Make the appropriate changes to the aaw-profile-state-controller repository. Add any unit tests to the changes in aaw-profile-state-controller to ensure correct behaviour. Depending on whether the exception kind is a pod/notebook capability or a namespace capability (described later on this page), the next steps differ. If the feature is a namespace capability, whatever controllers are involved with rolling out components for the capability should respond accordingly to changes in labels applied by aaw-profile-state-controller . For example, the cloud-main controller that permits authorized namespaces to connect to certain cloud-main services operates by creating certain Istio and Kubernetes resources in those namespaces to ensure that traffic from those namespaces is routed through the cloud main egress gateway. If a namespace adds an unauthorized user, the aaw-profile-state-controller applies the label state.aaw.statcan.gc.ca/exists-non-cloud-main-user: true , and the cloud-main controller and per-namespace network policy controller remove the necessary Istio and Kubernetes components from that namespace. As long as the unauthorized user is in the namespace, traffic from that namespace does not get routed through the cloud-main egress gateway, and is subsequently blocked at the AAW Hub firewall level if an attempt is made to communicate with cloud main services. If the feature is a pod/notebook capability, there are often two cases that need to be handled explicitly by a Gatekeeper policy: A namespace already contains a pod with feature X and an unauthorized user is added to the namespace. A namespace already contains an unauthorized user and a pod/notebook with feature X is created in the namespace. A Gatekeeper policy/constraint must be created for both cases so that the above scenarios are blocked at validating admission control (i.e. before the pod/notebook/rolebinding is posed to etcd). In addition, it may also be necessary to make changes to the jupyter-apis front-end to indicate on the UI that users are not allowed to perform certain actions. For example, for the SAS Notebook feature of the platform, the option to select a SAS notebook through the jupyter-apis UI is disabled if there exists a user without the SAS Notebook capability in the namespace.","title":"How to Add a New Capability"},{"location":"features/rbac/non-employee-rbac/#feature-deployment","text":"The aaw-profile-state-controller is deployed in the daaas-system ArgoCD application. The manifests for the aaw-profile-state-controller deployment can be found in the daaas-system folder in the aaw-argocd-manifests repository. The [aaw-profile-state-controller] repository has a github action that builds and pushes the image for the profile-state-controller application to our AAW artifactory instance. The container image is tagged with the SHA of the commit on the master branch that triggered the github action. To update the image, go to the aaw-argocd-manifests/daaas-system/profile-state-controller/deployment.yaml and update the image tag to contain the SHA of the most recent commit in the aaw-profile-state-controller repo.","title":"Feature Deployment"},{"location":"features/rbac/non-employee-rbac/#relevant-pull-requests","text":"This section shows the relevant pull requests that were involved with rolling out this feature into the AAW dev environment. Future updates to this feature will likely require similar pull requests to those shown below.","title":"Relevant Pull Requests"},{"location":"features/rbac/non-employee-rbac/#feature-implementation","text":"This implementation proposes two kinds of feature capabilities: (1) Pod/Notebook features and (2) Namespace features. For example, the SAS notebook feature is a Pod/notebook feature because it requires deploying a specific kind of notebook into a namespace, whereas cloud-main-connectivity is a namespace feature because routing pod traffic through the egress gateway is determined by rules that are applied at the level of the namespace. The semantics for labels behind each feature work as follows.","title":"Feature Implementation"},{"location":"features/rbac/non-employee-rbac/#podnotebook-feature","text":"The profile state controller applies a label of the form has-X-feature if any pod/notebook in the namespace has that feature (e.g. a pod with the sas image). Additionally, the profile state controller applies a label of the form exists-non-X-user if any subject in any rolebinding in the namespace is not an employee or not in the list of exceptions for that capability. The profiles state controller will apply the following labels to the profile and namespace: has-X-feature: true if any pod in the namespace has that feature exists-non-X-user: true if any subject in any role binding is not an employee or is not in the exception list for that capability. If a non-employee without an exception is added to a rolebinding in a namespace where the label has-X-feature: true is present, a gatekeeper policy blocks this request. If a pod with X-feature is added to a namespace where the label exists-non-X-user is present, a gatekeeper policy blocks this request.","title":"Pod/Notebook Feature"},{"location":"features/rbac/non-employee-rbac/#namespace-feature","text":"A namespace feature only requires the logic surrounding the exists-non-X-user label described above. In the case of cloud-main connectivity, the cloud-main and network controllers in aaw-kubeflow-profiles-controller should automatically reconcile the network policies / virtual services involved if a namespace does not have the cloud main connectivity capability. No gatekeeper policy should be required.","title":"Namespace Feature"},{"location":"features/source-control/gitea/","text":"Overview The platform uses Gitea for multitenant (per-namespace) source control. Each namespace that opts-in for source control gets its own dedicated Gitea instances - one for unclassified data, and one for protected-b data. Add Gitea Deployment to Kubeflow Profile We have created an addGitea jsonnet function in the aaw-kubeflow-profiles repository. See this aaw-kubeflow-profiles example for how to add Gitea to a Kubeflow profile. Feature Deployment Per-namespace Gitea is handled through the aaw-kubeflow-profiles-controller repository; see the README.md of this repository for documentation about how the aaw-kubeflow-profiles-controller is updated and deployed. Responsible for deploying gitea as argocd applications per Profile . Currently, argocd applications are deployed by the gitea controller based on the customized gitea manifest found here . The diagram below highlights the key components involved with the Gitea controller. Infrastructure Deployment Related PRs : Deploy Profiles Automation ArgoCD App Add Postgresql variables and secrets","title":"Source Control"},{"location":"features/source-control/gitea/#overview","text":"The platform uses Gitea for multitenant (per-namespace) source control. Each namespace that opts-in for source control gets its own dedicated Gitea instances - one for unclassified data, and one for protected-b data.","title":"Overview"},{"location":"features/source-control/gitea/#add-gitea-deployment-to-kubeflow-profile","text":"We have created an addGitea jsonnet function in the aaw-kubeflow-profiles repository. See this aaw-kubeflow-profiles example for how to add Gitea to a Kubeflow profile.","title":"Add Gitea Deployment to Kubeflow Profile"},{"location":"features/source-control/gitea/#feature-deployment","text":"Per-namespace Gitea is handled through the aaw-kubeflow-profiles-controller repository; see the README.md of this repository for documentation about how the aaw-kubeflow-profiles-controller is updated and deployed. Responsible for deploying gitea as argocd applications per Profile . Currently, argocd applications are deployed by the gitea controller based on the customized gitea manifest found here . The diagram below highlights the key components involved with the Gitea controller.","title":"Feature Deployment"},{"location":"features/source-control/gitea/#infrastructure-deployment","text":"Related PRs : Deploy Profiles Automation ArgoCD App Add Postgresql variables and secrets","title":"Infrastructure Deployment"}]}